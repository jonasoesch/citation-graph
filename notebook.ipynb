{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation Graph\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Ausgangslage\n",
    "\n",
    "Zu  Beginn  jeder  Forschungarbeitmuss eine Übersicht über die beste-hende Literatur erarbeitet werden. DieForschenden  suchen  dazu  Papers und Bücher über Suchmaschinen wie Google Scholar oder folgen den Zitaten in der Literatur die sie bereits haben. Dieser Prozess ist nicht immer einfach,  da  damit  nicht  immer  alle wichtigen Publikationen für ein gegebenes Forschungsfeld gefunden werden. \n",
    "\n",
    "## Ziel der Arbeit\n",
    "\n",
    "Das Ziel der Arbeit ist ein Werkzeug zu entwickeln, dass Forschenden hilft die bestehende Literatur um ein Thema systematisch zu erkunden und es auf ihre Bedürnisse anzupassen. Ausgehend von einer Auswahl relevanter Literatur und von geeigneten Stichwörtern wird über Referenzen ein geeigneter Graph erstellt, der interaktiv und mit Hilfe von NLP Verfahren auf die spezifischen Bedürfnissedes Forschenden angepasst werden soll.\n",
    "\n",
    "## Problemstellung\n",
    "\n",
    "* *User Interface:* Zur Erstellung, Visualisierung, Navigation und interaktive Anpassung des Graphen. Für die Erstellung des Graphen kann z.B. das Microsoft Academic Graph API verwendet werden.\n",
    "\n",
    "* *Graphanalyse:* Anwendung verschiedener Methoden zur Graphanalyse um Vorschläge für weitere Literatur zu machen. \n",
    "\n",
    "* *User-Interaktion:* Um die Suche im Graphen zu verfeinern und besser auf die spezifische Situation des Forschenden anzupassen.\n",
    "\n",
    "* *Topic Modelling:* Anwendung von Clustering-Algorithmen und Topic-Modelling der Abstracts um Themen in der Literatur zu identifizieren.\n",
    "\n",
    "## Technologien/Fachliche Schwerpunkte/Referenzen\n",
    "\n",
    "* Datenanalyse mit Python, Pandas und iGraph und NLTK\n",
    "* Nutzung der Microsoft Academic Graph API\n",
    "* Einfache Web-Applikation zum Beispiel mit Flask (Python), HTML, CSS und JavaScript\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a database of papers that are connected to a list of \"seed papers\". Metainformation of all papers that are cited in the *seed papers* is downloaded. Also, the metainformation for all papers that cite the seed papers is downloaded.\n",
    "\n",
    "The final \"database\" is in JSON-format and looks like this:\n",
    "\n",
    "~~~json\n",
    "{\n",
    "  \"21202130\": {\n",
    "    \"AA\": [\n",
    "      {\n",
    "        \"S\": 1,\n",
    "        \"AuId\": 2146551188,\n",
    "        \"AfN\": \"ohio state university\",\n",
    "        \"AfId\": 52357470,\n",
    "        \"AuN\": \"james moody\"\n",
    "      },\n",
    "      …\n",
    "    ],\n",
    "    \"DN\": \"Dynamic network visualization\",\n",
    "    \"V\": 110,\n",
    "    \"ECC\": 449,\n",
    "    \"D\": \"2005-01-01\",\n",
    "    \"F\": [\n",
    "      {\n",
    "        \"FId\": 62886766,\n",
    "        \"FN\": \"organizational network analysis\"\n",
    "      },\n",
    "      …\n",
    "    ],\n",
    "    \"CC\": 261,\n",
    "    \"LP\": 1241,\n",
    "    \"J\": {\n",
    "      \"JN\": \"amer j sociol\",\n",
    "      \"JId\": 122471516\n",
    "    },\n",
    "    \"L\": \"en\",\n",
    "    \"FP\": 1206,\n",
    "    \"IA\": {\n",
    "      \"IndexLength\": 129,\n",
    "      \"InvertedIndex\": {\n",
    "        \"“movies.”\": [\n",
    "          23\n",
    "        ],\n",
    "        \"particularly\": [\n",
    "          89\n",
    "        ],\n",
    "        \"over\": [\n",
    "          70\n",
    "        ],\n",
    "    },\n",
    "    \"S\": [\n",
    "      {\n",
    "        \"U\": \"http://www.jstor.org/stable/10.1086/421509\",\n",
    "        \"Ty\": 3\n",
    "      },\n",
    "   …\n",
    "    ],\n",
    "    \"VFN\": \"American Journal of Sociology\",\n",
    "    \"W\": [\n",
    "      \"dynamic\",\n",
    "      \"network\",\n",
    "      \"visualization\"\n",
    "    ],\n",
    "    \"Ti\": \"dynamic network visualization\",\n",
    "    \"Y\": 2005,\n",
    "    \"RId\": [\n",
    "      2112090702,\n",
    "      2148606196,\n",
    " …\n",
    "    ],\n",
    "    \"logprob\": -17.963,\n",
    "    \"Id\": 21202130,\n",
    "    \"I\": 4\n",
    "  },\n",
    "  …\n",
    "\n",
    "~~~\n",
    "\n",
    "The downloader uses the Microsoft Academic Graph API to find papers by title or by their Id like this:\n",
    "\n",
    "~~~\n",
    "https://api.labs.cognitive.microsoft.com/academic/v1.0/evaluate?expr=Ti='iterating between tools to create and edit visualizations'&model=latest&count=10&offset=0&attributes=Id,Ti,L,Y,D,CC,ECC,AA.AuN,AA.AuId,AA.AfN,AA.AfId,AA.S,F.FN,F.FId,J.JN,C.CId,RId,W,E.DN,E.S,E.S.Ty,E.DOI\n",
    "~~~\n",
    "\n",
    "In order to query the API, you need to create your own key for [Microsoft Cognitive](https://labs.cognitive.microsoft.com/en-us/project-academic-knowledge) and add it to `config.py`\n",
    "\n",
    "## Open Academic Graph\n",
    "\n",
    "Long term, it would be better not to rely on a rate-limited API graciously provided by Microsoft. Fortunately, the are nowadays not evil and provide everything as [Open Data](https://www.openacademic.ai/oag/). Would need get that into a Graph Database et voilà."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import config\n",
    "from copy import deepcopy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the paper database\n",
    "db = {}\n",
    "with open('db.json', 'r') as db_json:\n",
    "    db = json.load(db_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(name):\n",
    "    name = name.lower()\n",
    "    name = name.replace(\"-\", \" \")\n",
    "    name = name.replace(\".\", \"\")\n",
    "    name = name.replace(\"?\", \"\")\n",
    "    name = name.replace(\":\", \"\")\n",
    "    name = name.replace(\",\", \"\")\n",
    "    name = name.replace(\"-\", \" \")\n",
    "    name = re.sub(r\"\\s\\s+\", \" \", name)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_for_name(name):\n",
    "        normalized = normalize(name)\n",
    "        url = 'https://api.labs.cognitive.microsoft.com/academic/v1.0/evaluate'\n",
    "        params = {\n",
    "            'expr': \"Ti='\"+normalized+\"'\",\n",
    "            'model': 'latest',\n",
    "            'count': 10,\n",
    "            'offset': 0,\n",
    "            'attributes': 'Id,Ti,L,Y,D,CC,ECC,AA.AuN,AA.AuId,AA.AfN,AA.AfId,AA.S,F.FN,F.FId,J.JN,J.JId,C.CN,C.CId,RId,W,E.DN,E.S,E.VFN,E.VSN,E.V,E.I,E.FP,E.LP,E.DOI,E.CC,E.IA'\n",
    "        }\n",
    "        headers = {'Ocp-Apim-Subscription-Key': config.api_key}\n",
    "        r = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "        result = r.json()\n",
    "\n",
    "        if 'entities' in result:\n",
    "            return str(result['entities'][0]['Id'])\n",
    "        else:\n",
    "            print(id)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some useful functions to query the Microsoft Academic Graph API\n",
    "def download_for_id(id):\n",
    "    url = 'https://api.labs.cognitive.microsoft.com/academic/v1.0/evaluate'\n",
    "    params = {\n",
    "        'expr': 'Id='+str(id),\n",
    "        'model': 'latest',\n",
    "        'count': 10,\n",
    "        'offset': 0,\n",
    "        'attributes': 'Id,Ti,L,Y,D,CC,ECC,AA.AuN,AA.AuId,AA.AfN,AA.AfId,AA.S,F.FN,F.FId,J.JN,J.JId,C.CN,C.CId,RId,W,E.DN,E.S,E.VFN,E.VSN,E.V,E.I,E.FP,E.LP,E.DOI,E.CC,E.IA'\n",
    "    }\n",
    "    headers = {'Ocp-Apim-Subscription-Key': config.api_key}\n",
    "\n",
    "    r = requests.get(url, params=params, headers=headers)\n",
    "    \n",
    "    result = r.json()\n",
    "    if 'entities' in result:\n",
    "        return r.json()['entities'][0]\n",
    "    else:\n",
    "        print(id)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_refs(id):\n",
    "    url = 'https://api.labs.cognitive.microsoft.com/academic/v1.0/evaluate'\n",
    "    params = {\n",
    "        'expr': 'RId='+str(id),\n",
    "        'model': 'latest',\n",
    "        'count': 10000,\n",
    "        'offset': 0,\n",
    "        'attributes': 'Id,Ti,L,Y,D,CC,ECC,AA.AuN,AA.AuId,AA.AfN,AA.AfId,AA.S,F.FN,F.FId,J.JN,J.JId,C.CN,C.CId,RId,W,E.DN,E.S,E.VFN,E.VSN,E.V,E.I,E.FP,E.LP,E.DOI,E.CC,E.IA'\n",
    "    }\n",
    "    headers = {'Ocp-Apim-Subscription-Key': config.api_key}\n",
    "\n",
    "    r = requests.get(url, params=params, headers=headers)\n",
    "    result = r.json()\n",
    "\n",
    "    if 'entities' in result:\n",
    "        return r.json()['entities']\n",
    "    else:\n",
    "        print(id)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Identify seed papers\n",
    "\n",
    "First we define a list of seed papers. We found the ID's manually through the Academic Graph API like this:\n",
    "\n",
    "~~~\n",
    "https://westus.api.cognitive.microsoft.com/academic/v1.0/evaluate?expr=Ti='iterating between tools to create and edit visualizations'\n",
    "~~~\n",
    "\n",
    "Some considerations:\n",
    "* You first need to register for Microsoft Azure Cognitive Services and get a \"Ocp-Apim-Subscription-Key\". This key needs to be sent as a header with your request. Microsoft limits the quota that is available for free\n",
    "\n",
    "* The paper title defined in the \"Ti\"-field need to be all lowercase and is stripped of all special characters like \":\" or \"–\". Otherwise the resulting response will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_papers =[\n",
    "    id_for_name(\"Animated transitions in statistical data graphics\"),\n",
    "    id_for_name(\"A deeper understanding of sequence in narrative visualization\"),\n",
    "    id_for_name(\"Toward a Deeper Understanding of the Role of Interaction in Information Visualization\"),\n",
    "    id_for_name(\"Understanding data videos: Looking at narrative visualization through the cinematography lens\"),\n",
    "    id_for_name(\"Animations 25 Years Later: New Roles and Opportunities\"),\n",
    "    id_for_name(\"Authoring Narrative Visualizations with Ellipsis\"),\n",
    "    id_for_name(\"The Not-so-Staggering Effect of Staggered Animated Transitions on Visual Tracking\"),\n",
    "    id_for_name(\"Animation - can it facilitate?\"),\n",
    "    id_for_name(\"Display of Key Pictures from Animation: Effects on Learning\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2125215841',\n",
       " '1980209219',\n",
       " '2161133721',\n",
       " '2143880496',\n",
       " '2408736664',\n",
       " '1530372876',\n",
       " '1989510471',\n",
       " '2108075477',\n",
       " '53303969']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download the information of the seed papers\n",
    "\n",
    "Download the metadata (including the citations) for the seed papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already in DB\n",
      "Already in DB\n",
      "Already in DB\n",
      "Already in DB\n",
      "Already in DB\n",
      "Already in DB\n",
      "Already in DB\n",
      "Already in DB\n",
      "Already in DB\n"
     ]
    }
   ],
   "source": [
    "# 2. Download the information of the seed papers\n",
    "papers = {}\n",
    "for paper_id in seed_papers:\n",
    "    if not (paper_id in db): # Only do a request if paper is not already in DB\n",
    "        db[str(paper_id)] = download_for_id(str(paper_id))\n",
    "    else:\n",
    "        print('Already in DB')\n",
    "    papers[str(paper_id)] = db[str(paper_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save a copy of the database to JSON\n",
    "\n",
    "Save the result which helps to stay in the quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('db.json', 'w') as outfile:\n",
    "    json.dump(db, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrive info of all papers cited in seed papers\n",
    "\n",
    "This is the longest step which can take quite a few API-requests. We try to not do them twice and also added a rate limit to stay friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "In DB\n",
      "downloaded: 2037124948\n",
      "downloaded: 2337068785\n",
      "downloaded: 2107031757\n",
      "downloaded: 2137145004\n",
      "In DB\n",
      "downloaded: 2125259774\n",
      "downloaded: 2001467963\n",
      "downloaded: 1706881574\n",
      "downloaded: 2000885037\n",
      "downloaded: 2032152873\n",
      "downloaded: 2140996113\n",
      "downloaded: 2061177236\n",
      "downloaded: 2014654802\n",
      "In DB\n",
      "downloaded: 2103857331\n",
      "downloaded: 1860905340\n",
      "downloaded: 1729578042\n",
      "In DB\n",
      "downloaded: 2108309120\n",
      "In DB\n",
      "In DB\n",
      "downloaded: 617486778\n",
      "downloaded: 2124818066\n",
      "In DB\n",
      "downloaded: 1986262602\n",
      "In DB\n",
      "downloaded: 2171111636\n",
      "downloaded: 2152813220\n",
      "In DB\n",
      "downloaded: 2083009591\n",
      "downloaded: 122253890\n",
      "In DB\n",
      "downloaded: 1982585917\n",
      "In DB\n",
      "downloaded: 614773412\n",
      "downloaded: 1984227063\n",
      "downloaded: 1817419736\n",
      "In DB\n",
      "In DB\n",
      "downloaded: 2088862522\n",
      "In DB\n",
      "In DB\n",
      "downloaded: 181374097\n"
     ]
    }
   ],
   "source": [
    "papers_clone = deepcopy(papers)\n",
    "for paper in papers:\n",
    "    if paper in seed_papers:\n",
    "        if not('RId' in papers[paper]):\n",
    "            None     # Otherwise we'll get all the citations from other papers downloaded previously\n",
    "        else:\n",
    "            for citation in papers[paper]['RId']:\n",
    "                if (str(citation) in db):\n",
    "                    print('In DB')\n",
    "                else:\n",
    "                    db[str(citation)] = download_for_id(str(citation))\n",
    "                    print('downloaded: '+str(citation))\n",
    "                    time.sleep(0.4)\n",
    "                papers_clone[str(citation)] = db[str(citation)]\n",
    "            \n",
    "papers = papers_clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to DB again\n",
    "with open('db.json', 'w') as outfile:\n",
    "    json.dump(papers, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Retrieve all papers which cite the seed papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in seed_papers:\n",
    "    # We do it in all cases because it only uses as many requests\n",
    "    # as we have seed papers -> inexpensive\n",
    "    referring = find_all_refs(paper)\n",
    "    for ref in referring:\n",
    "        db[str(ref['Id'])] = ref\n",
    "        papers[str(ref['Id'])] = ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('db.json', 'w') as outfile:\n",
    "    json.dump(db, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979 1979\n"
     ]
    }
   ],
   "source": [
    "print(len(db), len(papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import igraph\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_seeds = ['565', '188', '513', '747', '831']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = igraph.Graph()\n",
    "G.to_directed()\n",
    "paperli = []\n",
    "for paper in papers:\n",
    "    if not(paper in anti_seeds):\n",
    "        paperli.append(paper)\n",
    "\n",
    "G.add_vertices(len(paperli))\n",
    "\n",
    "for i, paper in enumerate(paperli):\n",
    "    if 'RId' in papers[paper]:\n",
    "        for citation in papers[paper]['RId']:\n",
    "            for j, subpaper in enumerate(paperli):\n",
    "                if str(subpaper) == str(citation):\n",
    "                    G.add_edges([(i, j)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, paper in enumerate(paperli):\n",
    "    G.vs[i][\"id\"] = papers[paper]['Id']\n",
    "    G.vs[i][\"citations\"] = papers[paper]['CC']\n",
    "    G.vs[i][\"author\"] = papers[paper]['AA'][0][\"AuN\"]\n",
    "    G.vs[i][\"year\"] = papers[paper]['Y']\n",
    "    G.vs[i][\"name\"] = papers[paper]['DN']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043478260869565216"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.es[\"weight\"] = 0\n",
    "for vertex in G.vs:\n",
    "    for edge in G.es.select(_source_eq=vertex.index):\n",
    "        G.es[edge.index][\"weight\"] = G.es[edge.index][\"weight\"] + vertex[\"citations\"]\n",
    "for vertex in G.vs:\n",
    "    for edge in G.es.select(_target_eq=vertex.index):\n",
    "        G.es[edge.index][\"weight\"] = G.es[edge.index][\"weight\"] / vertex[\"citations\"]\n",
    "\n",
    "G.es[0][\"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.vs[\"pagerank\"] = G.pagerank(weights=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "pr = pd.DataFrame(list(zip(G.vs[\"id\"], G.vs[\"author\"], G.vs[\"name\"], G.vs[\"pagerank\"], G.vs[\"citations\"])), columns=[\"id\", \"author\", \"name\", \"pr\", \"citations\"])\n",
    "pr.sort_values(\"pr\", ascending=False).to_csv('pagerank.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Relative importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.ecount()\n",
    "arr = []\n",
    "for v in G.vs:\n",
    "    arr.append([v[\"author\"], v[\"year\"], v[\"name\"], v.indegree(), v['citations'], v[\"pagerank\"]])\n",
    "inner_importance = pd.DataFrame(arr, columns=(\"author\", \"year\", \"name\", \"deg\", \"citations\", \"pagerank\"))\n",
    "inner_importance[\"rating\"] = (inner_importance[\"deg\"] / inner_importance[\"citations\"]) * (inner_importance[\"year\"]-1900)\n",
    "inner_importance = inner_importance[inner_importance['citations']>10]\n",
    "inner_importance = inner_importance[inner_importance['deg']>1]\n",
    "inner_importance = inner_importance.sort_values(\"rating\", ascending=False)\n",
    "\n",
    "inner_importance.to_csv('reading.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizing the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2f9401029e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_graphml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'papers_graph.graphml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "G.write_graphml('papers_graph.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gephi](https://gephi.org/) can be used for a first visualization of the graph. It's interesting to try and define clusters:\n",
    "\n",
    "![](gephi.png)\n",
    "\n",
    "There are also clustering options in igraph. Clusters can also be visualized an compared as a Hiveplot:\n",
    "\n",
    "![](hiveplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interesting projects\n",
    "\n",
    "* [Headstart](https://github.com/OpenKnowledgeMaps/Headstart/blob/master/README.md)\n",
    "* [PivotPaths](https://github.com/ShreenathIyer/pivot-paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
